{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acoustic-bulgaria",
   "metadata": {},
   "source": [
    "# 1. 기존 방법론에 대한 비판\n",
    "**LSA**는 DTM/TF-IDF 빈도수 카운트 행렬을 truncated SVD해 잠재 의미 도출   \n",
    "전체적 통계 정보를 고려하나, 단어 의미 유추 작업에는 성능이 떨어짐   \n",
    "**Word2Vec**은 예측값과 실제값 오차를 손실 함수로 줄여나가며 학습   \n",
    "유추 작업에는 뛰어나나, 윈도우에서만 주변 단어를 고려해 전체적 통계 정보를 반영 못함   \n",
    "**Glove**는 카운트와 예측 기반을 모두 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-dealer",
   "metadata": {},
   "source": [
    "# 2.윈도우 기반 동시 등장 행렬(Window based Co-occurence Matrix)\n",
    "전체 단어 집합으로 행렬 구성   \n",
    "\n",
    "카운트|I|like|enjoy|deep|learning|NLP|flying\n",
    ":--|:--|:--|:--|:--|:--|:--|:--\n",
    "I|0|2|1|0|0|0|0\n",
    "like|2|0|0|1|0|1|0\n",
    "enjoy|1|0|0|0|0|0|1\n",
    "deep|0|1|0|0|1|0|0\n",
    "learning|0|0|0|1|0|0|0\n",
    "NLP|0|1|0|0|0|0|0\n",
    "flying|0|0|1|0|0|0|0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-colors",
   "metadata": {},
   "source": [
    "# 3. 동시 등장 확률(Co-occurence Probability)\n",
    "동시 등장 행렬에서 특정 단어의 전체 등장 횟수와, 그 특정 단어가 등장할 때 어떤 단어가 등장한 횟수를 계산한 조건부 확률\n",
    "\n",
    "동시 등장 확률과 크기 관계 비|k=solid|k=gas|k=water|k=fashion\n",
    ":--|:--|:--|:--|:--\n",
    "P(ice가 등장할 때 k의 확률)|큰 값|작은 값|큰 값|작은 값\n",
    "P(steam이 등장할 때 k의 확률)|작은 값|큰 값|큰 값|작은 값\n",
    "P(ice가 등장할 때 k의 확률) / P(steam이 등장할 때 k의 확률)|큰 값|작은 값|1에 가까움|1에 가까움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-viking",
   "metadata": {},
   "source": [
    "# 4. 손실 함수(Loss function)\n",
    "GloVe는 임베딩된 중심 단어와 주변 단어의 내적이, 전체 corpus에서 동시 등장 확률임\n",
    "$$w_{i} \\cdot \\tilde{w_{k}} \\approx P(k\\,|\\,i) = P_{ik}$$\n",
    "\n",
    "벡터들로 함수 F 적용시 동시 등장 확률 비가 나온다는 초기 식 가정\n",
    "$$F(w_{i}, w_{j}, \\tilde{w_{k}}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "F는 동시 등장 확률 비를 벡터 공간에 인코딩하는 목적\n",
    "$$F(w_{i} - w_{j}, \\tilde{w_{k}}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "좌변은 벡터고 우변은 스칼라임을 성립해주는 내적   \n",
    "(정리하자면 선형 공간에서 단어 의미 관계를 표현하고자 뺄셈과 내적 사용)\n",
    "$$F((w_{i} - w_{j})^{T}\\tilde{w_{k}}) = \\frac{P_{ik}}{P_{jk}}$$   \n",
    "\n",
    "이때 F는 실수 덧셈과 양수 곱셈에서 준동형(homomorphism)을 만족   \n",
    "$$F(a + b) = F(a)F(b),\\,\\forall{a},\\,b \\in \\mathbb{R}$$\n",
    "$$F(v_{1}^{T}v_{2} + v_{3}^{T}v_{4}) = F(v_{1}^{T}v_{2})F(v_{3}^{T}v_{4}),\\,\\forall{v_{1},\\,v_{2},\\,v_{3},\\,v_{4}} \\in V$$\n",
    "$$F(v_{1}^{T}v_{2} - v_{3}^{T}v_{4}) = \\frac{F(v_{1}^{T}v_{2})}{F(v_{3}^{T}v_{4})},\\,\\forall{v_{1},\\,v_{2},\\,v_{3},\\,v_{4}} \\in V$$\n",
    "\n",
    "이를 GloVe에 적용\n",
    "$$F((w_{i} - w_{j})^{T}\\tilde{w_{k}}) = \\frac{F(w_{i}^{T}\\tilde{w_{k}})}{F(w_{j}^{T}\\tilde{w_{k}})} = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "이를 만족하는 F는 exponential function\n",
    "$$exp(w_{i}^{T}\\tilde{w_{k}} - w_{j}^{T}\\tilde{w_{k}}) = \\frac{exp(w_{i}^T\\tilde{w_{k}})}{exp(w_{j}^T\\tilde{w_{k}})}$$\n",
    "$$exp(w_{i}^{T}\\tilde{w_{k}}) = P_{ik} = \\frac{X_{ik}}{X_{i}}$$\n",
    "$$w_{i}^{T}\\tilde{w_{k}} = logP_{ik} = log(\\frac{X_{ik}}{X_{i}}) = logX_{ik} - logX_{i}$$\n",
    "\n",
    "각 임베딩 벡터의 위치를 바꾸어도 식이 성립해야 하는데 로그항이 걸림돌로 작용하므로, 로그항을 편향으로 대체\n",
    "$$w_{i}^{T}\\tilde{w_{k}} + b_{i} + \\tilde{b_{k}} = logX_{ik}$$\n",
    "\n",
    "손실 함수는 이 차이를 최소화하는 방향으로 일반화    \n",
    "로그항이 0이 될 수 있어 log(1+X)로 사용하기도 함\n",
    "$$Loss function = \\sum_{m,n=1}^{V}(w_{m}^{T} + b_{m} + \\tilde{b_{n}} - logX_{mn})^{2}$$\n",
    "   \n",
    "희소 행렬에서 낮은 빈도수에 가중치를 주는 weight function 도입, X가 커질 때 지나친 가중치를 막고자 최댓값 설정   \n",
    "$$f(x) = min(1, (\\frac{x}{x_{max}})^{\\frac{3}{4}})$$\n",
    "![weight function](https://wikidocs.net/images/page/22885/%EA%B0%80%EC%A4%91%EC%B9%98.PNG \"weight function\")\n",
    "\n",
    "최종 손실 함수\n",
    "$$Loss function = \\sum_{m,n=1}^{V}\\,f(X_{mn})(w_{m}^{T}\\tilde{w_{n}} + b_{m} + \\tilde{b_{n}} - logX_{mn})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-administrator",
   "metadata": {},
   "source": [
    "# 5. GloVe 훈련시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "therapeutic-buddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x1e5cecf0e20>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\",\n",
    "                           filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legislative-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorrect-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 20 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n"
     ]
    }
   ],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "# 훈련 데이터로부터 동시 등장 행렬 생성\n",
    "corpus = Corpus()\n",
    "corpus.fit(result, window=5)\n",
    "\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recognized-layer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9648368487300606), ('guy', 0.87853441458273), ('girl', 0.8534987437931901), ('young', 0.846071380470317)]\n"
     ]
    }
   ],
   "source": [
    "model_result1 = glove.most_similar(\"man\")\n",
    "print(model_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adequate-junction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 0.9476923851537493), ('woman', 0.8419994258815078), ('kid', 0.8361017051820097), ('man', 0.8279171755525907)]\n"
     ]
    }
   ],
   "source": [
    "model_result2 = glove.most_similar(\"boy\")\n",
    "print(model_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suburban-specification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('harvard', 0.888141053853674), ('mit', 0.8571900561051062), ('cambridge', 0.8353162335157012), ('stanford', 0.8343890928881722)]\n"
     ]
    }
   ],
   "source": [
    "model_result3 = glove.most_similar(\"university\")\n",
    "print(model_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "willing-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('clean', 0.8436805363087417), ('air', 0.8276533018109319), ('electricity', 0.818030617567804), ('fresh', 0.8173633565603985)]\n"
     ]
    }
   ],
   "source": [
    "model_result4 = glove.most_similar(\"water\")\n",
    "print(model_result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arctic-setting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chemistry', 0.8938172020985033), ('economics', 0.8825295341379408), ('beauty', 0.8640229385013825), ('mathematics', 0.8553610296644241)]\n"
     ]
    }
   ],
   "source": [
    "model_result5 = glove.most_similar(\"physics\")\n",
    "print(model_result5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ongoing-quantity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tissue', 0.8488022951766478), ('nerve', 0.836366071812534), ('stem', 0.7888055281526625), ('channel', 0.7717473953205604)]\n"
     ]
    }
   ],
   "source": [
    "model_result6 = glove.most_similar(\"muscle\")\n",
    "print(model_result6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brief-combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('water', 0.8436805363087417), ('fresh', 0.8321380109071955), ('wind', 0.8010232560853364), ('heat', 0.7796518615106788)]\n"
     ]
    }
   ],
   "source": [
    "model_result7 = glove.most_similar(\"clean\")\n",
    "print(model_result7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
