{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "complex-sunday",
   "metadata": {},
   "source": [
    "# 1. 특이값 분해(Singular Value Decomposition, SVD)\n",
    "A가 m x n 행렬일 때 이를 3개의 행렬 곱으로 분해   \n",
    "U와 VT는 직교행렬, 시그마는 대각행렬\n",
    "$$A = U\\sum V^{T}$$\n",
    "\n",
    "> **1) 전치행렬(Transposed Matrix)**   \n",
    "> 기존 행렬에서 행과 열을 바꿈\n",
    "> $$M = \\begin{bmatrix} 1&2\\\\3&4\\\\5&6\\\\ \\end{bmatrix},\\, M^{T} = \\begin{bmatrix} 1&3&5\\\\2&4&6\\\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "american-controversy",
   "metadata": {},
   "source": [
    "> **2) 단위행렬(Identity Matrix)**   \n",
    "> 주대각선 원소가 모두 1, 나머지는 0   \n",
    "> $$I = \\begin{bmatrix} 1&0&0\\\\0&1&0\\\\0&0&1 \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-emerald",
   "metadata": {},
   "source": [
    "> **3) 역행렬(Inverse Matrix)**   \n",
    "> 행렬과 역행렬을 곱할 때 단위행렬이 됨\n",
    "> $$A \\times A^{-1} = I$$   \n",
    "> $$\\begin{bmatrix} 1&2&3\\\\4&5&6\\\\7&8&9\\ \\end{bmatrix} \\times \\begin{bmatrix} ? \\end{bmatrix} = \\begin{bmatrix} 1&0&0\\\\0&1&0\\\\0&0&1\\ \\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-account",
   "metadata": {},
   "source": [
    "> **4) 직교행렬(Orthogonal Matrix)**   \n",
    "> 자신과 자신의 전치 행렬 곱 혹은 그 반대 결과가 단위 행렬이 됨\n",
    "> $$A \\times A^{T} = I,\\, A^{T} \\times A = I$$   \n",
    "> $$A^{-1} = A^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corporate-cameroon",
   "metadata": {},
   "source": [
    "> **5) 대각행렬(Diagonal Matrix)**   \n",
    "> 주대각선에 속하지 않은 원소가 모두 0   \n",
    "> 특이값(singular value)은 내림차순 정렬(a > b > c)\n",
    "> $$\\sum = \\begin{bmatrix} a&0&0\\\\0&b&0\\\\0&0&c\\ \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "technological-dream",
   "metadata": {},
   "source": [
    "# 2. 절단된 SVD\n",
    "![Truncated SVD](https://miro.medium.com/max/398/0*dLoOJxagJw9Fwrfq.PNG \"Truncated SVD\")\n",
    "\n",
    "truncated SVD는 대각행렬 상위 t개만 남으며 값이 손실되어 기존 행렬로 복구 불가      \n",
    "t가 크면 기존 행렬의 다양한 의미를 가져가고, 작으면 노이즈를 제거함   \n",
    "영상 처리에서는 노이즈 제거, 자연어 처리에서는 설명력이 낮은 정보를 삭제하는 의미   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informational-server",
   "metadata": {},
   "source": [
    "# 3. 잠재 의미 분석(Latent Semantic Analysis, LSA)\n",
    "BoW에 기반한 DTM이나 TF-IDF는 단어 빈도수 수치화이므로 단어 의미 고려 불가   \n",
    "대안으로 잠재된(latent) 의미를 이끌어내는 잠재 의미 분석(Latent Semantic Analysis, LSA) 사용   \n",
    "LSA는 DTM이나 TF-IDF에 truncated SVD를 사용해 차원을 축소, 단어들의 잠재적 의미를 이끌어냄\n",
    "\n",
    "<null>|과일이|길고|노란|먹고|바나나|사과|싶은|저는|좋아요\n",
    ":--|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:\n",
    "문서1|0|0|0|1|0|1|1|0|0\n",
    "문서2|0|0|0|1|1|0|1|0|0\n",
    "문서3|0|1|1|0|2|0|0|0|0\n",
    "문서4|1|0|0|0|0|0|0|1|1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "thrown-generator",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.array([ [0,0,0,1,0,1,1,0,0],\n",
    "               [0,0,0,1,1,0,1,0,0],\n",
    "               [0,1,1,0,2,0,0,0,0],\n",
    "               [1,0,0,0,0,0,0,1,1] ])\n",
    "np.shape(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fewer-walnut",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full SVD 수행\n",
    "U, s, VT = np.linalg.svd(A, full_matrices=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "varying-intersection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.24  0.75  0.    0.62]\n",
      " [ 0.51  0.44 -0.   -0.74]\n",
      " [ 0.83 -0.49 -0.    0.27]\n",
      " [ 0.   -0.    1.   -0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 직교행렬 U\n",
    "print(U.round(2))\n",
    "np.shape(U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "vertical-affect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.69 2.05 1.73 0.77]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 대각행렬 S\n",
    "print(s.round(2))\n",
    "np.shape(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "chicken-voice",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.   0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   2.05 0.   0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   1.73 0.   0.   0.   0.   0.   0.  ]\n",
      " [0.   0.   0.   0.77 0.   0.   0.   0.   0.  ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S = np.zeros((4,9))\n",
    "S[:4, :4] = np.diag(s)\n",
    "\n",
    "print(S.round(2))\n",
    "np.shape(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "unauthorized-novel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.    0.31  0.31  0.28  0.8   0.09  0.28  0.    0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]\n",
      " [ 0.58 -0.    0.    0.   -0.    0.   -0.    0.58  0.58]\n",
      " [-0.    0.35  0.35 -0.16 -0.25  0.8  -0.16  0.    0.  ]\n",
      " [-0.   -0.78 -0.01 -0.2   0.4   0.4  -0.2   0.    0.  ]\n",
      " [-0.29  0.31 -0.78 -0.24  0.23  0.23  0.01  0.14  0.14]\n",
      " [-0.29 -0.1   0.26 -0.59 -0.08 -0.08  0.66  0.14  0.14]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19  0.75 -0.25]\n",
      " [-0.5  -0.06  0.15  0.24 -0.05 -0.05 -0.19 -0.25  0.75]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9, 9)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 직교행렬 VT\n",
    "print(VT.round(2))\n",
    "np.shape(VT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optional-measure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 기존 행렬로 복구\n",
    "np.allclose(A, np.dot(np.dot(U, S), VT).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-feature",
   "metadata": {},
   "source": [
    "truncated SVD에서 축소된 U의 행은 잠재 의미를 표현하고자 수치화된 각 문서 벡터   \n",
    "축소된 VT의 열은 잠재 의미를 표현하고자 수치화된 각 단어 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "corporate-routine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.69 0.  ]\n",
      " [0.   2.05]]\n"
     ]
    }
   ],
   "source": [
    "# truncated SVD 수행\n",
    "S = S[:2, :2]\n",
    "print(S.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "explicit-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.24  0.75]\n",
      " [ 0.51  0.44]\n",
      " [ 0.83 -0.49]\n",
      " [ 0.   -0.  ]]\n",
      "[[ 0.    0.31  0.31  0.28  0.8   0.09  0.28  0.    0.  ]\n",
      " [ 0.   -0.24 -0.24  0.58 -0.26  0.37  0.58 -0.   -0.  ]]\n"
     ]
    }
   ],
   "source": [
    "U = U[:, :2]\n",
    "print(U.round(2))\n",
    "\n",
    "VT = VT[:2, :]\n",
    "print(VT.round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "upper-vanilla",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 0 1 1 0 0]\n",
      " [0 0 0 1 1 0 1 0 0]\n",
      " [0 1 1 0 2 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 1 1]]\n",
      "[[ 0.   -0.17 -0.17  1.08  0.12  0.62  1.08 -0.   -0.  ]\n",
      " [ 0.    0.2   0.2   0.91  0.86  0.45  0.91  0.    0.  ]\n",
      " [ 0.    0.93  0.93  0.03  2.05 -0.17  0.03  0.    0.  ]\n",
      " [ 0.    0.    0.    0.    0.    0.    0.    0.    0.  ]]\n"
     ]
    }
   ],
   "source": [
    "# 기존 행렬로 복구\n",
    "A_prime = np.dot(np.dot(U, S), VT)\n",
    "print(A)\n",
    "print(A_prime.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-treatment",
   "metadata": {},
   "source": [
    "# 4. 실습을 통한 이해\n",
    "> **1) 뉴스그룹 데이터에 대한 이해**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "amateur-awareness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "specific-throat",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\n\\nYeah, do you expect people to read the FAQ, etc. and actually accept hard\\natheism?  No, you need a little leap of faith, Jimmy.  Your logic runs out\\nof steam!\\n\\n\\n\\n\\n\\n\\n\\nJim,\\n\\nSorry I can't pity you, Jim.  And I'm sorry that you have these feelings of\\ndenial about the faith you need to get by.  Oh well, just pretend that it will\\nall end happily ever after anyway.  Maybe if you start a new newsgroup,\\nalt.atheist.hard, you won't be bummin' so much?\\n\\n\\n\\n\\n\\n\\nBye-Bye, Big Jim.  Don't forget your Flintstone's Chewables!  :) \\n--\\nBake Timmons, III\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lonely-richmond",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n"
     ]
    }
   ],
   "source": [
    "print(dataset.target_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impossible-contract",
   "metadata": {},
   "source": [
    "> **2) 텍스트 전처리**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sunset-wagon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-14-4fba9e717a0e>:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document':documents})\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "challenging-aaron",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy your logic runs steam sorry pity sorry that have these feelings denial about faith need well just pretend that will happily ever after anyway maybe start newsgroup atheist hard bummin much forget your flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "christian-found",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "killing-franklin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yeah',\n",
       " 'expect',\n",
       " 'people',\n",
       " 'read',\n",
       " 'actually',\n",
       " 'accept',\n",
       " 'hard',\n",
       " 'atheism',\n",
       " 'need',\n",
       " 'little',\n",
       " 'leap',\n",
       " 'faith',\n",
       " 'jimmy',\n",
       " 'logic',\n",
       " 'runs',\n",
       " 'steam',\n",
       " 'sorry',\n",
       " 'pity',\n",
       " 'sorry',\n",
       " 'feelings',\n",
       " 'denial',\n",
       " 'faith',\n",
       " 'need',\n",
       " 'well',\n",
       " 'pretend',\n",
       " 'happily',\n",
       " 'ever',\n",
       " 'anyway',\n",
       " 'maybe',\n",
       " 'start',\n",
       " 'newsgroup',\n",
       " 'atheist',\n",
       " 'hard',\n",
       " 'bummin',\n",
       " 'much',\n",
       " 'forget',\n",
       " 'flintstone',\n",
       " 'chewables',\n",
       " 'bake',\n",
       " 'timmons']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_doc[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-calcium",
   "metadata": {},
   "source": [
    "> **3) TF-IDF 행렬 만들기**   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "moderate-authentication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 역토큰화\n",
    "detokenized_doc = []\n",
    "for i in range(len(news_df)):\n",
    "    t = ' '.join(tokenized_doc[i])\n",
    "    detokenized_doc.append(t)\n",
    "    \n",
    "news_df['clean_doc'] = detokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "straight-question",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yeah expect people read actually accept hard atheism need little leap faith jimmy logic runs steam sorry pity sorry feelings denial faith need well pretend happily ever anyway maybe start newsgroup atheist hard bummin much forget flintstone chewables bake timmons'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df['clean_doc'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "coordinate-billion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11314, 1000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english',\n",
    "                             max_features=1000,\n",
    "                             max_df=0.5, \n",
    "                             smooth_idf=True)\n",
    "\n",
    "X = vectorizer.fit_transform(news_df['clean_doc'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compatible-father",
   "metadata": {},
   "source": [
    "> **4) 토픽 모델링(Topic Modeling)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "stupid-bleeding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# 20개의 토픽을 가졌다고 가정\n",
    "svd_model = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=100, random_state=122)\n",
    "svd_model.fit(X)\n",
    "len(svd_model.components_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "official-matthew",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 1000)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.shape(svd_model.components_) # LSA에서 VT에 해당"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "naked-dating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1: [('like', 0.21386), ('know', 0.20046), ('people', 0.19293), ('think', 0.17805), ('good', 0.15128)]\n",
      "Topic 2: [('thanks', 0.32888), ('windows', 0.29088), ('card', 0.18069), ('drive', 0.17455), ('mail', 0.15111)]\n",
      "Topic 3: [('game', 0.37064), ('team', 0.32443), ('year', 0.28154), ('games', 0.2537), ('season', 0.18419)]\n",
      "Topic 4: [('drive', 0.53324), ('scsi', 0.20165), ('hard', 0.15628), ('disk', 0.15578), ('card', 0.13994)]\n",
      "Topic 5: [('windows', 0.40399), ('file', 0.25436), ('window', 0.18044), ('files', 0.16078), ('program', 0.13894)]\n",
      "Topic 6: [('chip', 0.16114), ('government', 0.16009), ('mail', 0.15625), ('space', 0.1507), ('information', 0.13562)]\n",
      "Topic 7: [('like', 0.67086), ('bike', 0.14236), ('chip', 0.11169), ('know', 0.11139), ('sounds', 0.10371)]\n",
      "Topic 8: [('card', 0.46633), ('video', 0.22137), ('sale', 0.21266), ('monitor', 0.15463), ('offer', 0.14643)]\n",
      "Topic 9: [('know', 0.46047), ('card', 0.33605), ('chip', 0.17558), ('government', 0.1522), ('video', 0.14356)]\n",
      "Topic 10: [('good', 0.42756), ('know', 0.23039), ('time', 0.1882), ('bike', 0.11406), ('jesus', 0.09027)]\n",
      "Topic 11: [('think', 0.78469), ('chip', 0.10899), ('good', 0.10635), ('thanks', 0.09123), ('clipper', 0.07946)]\n",
      "Topic 12: [('thanks', 0.36824), ('good', 0.22729), ('right', 0.21559), ('bike', 0.21037), ('problem', 0.20894)]\n",
      "Topic 13: [('good', 0.36212), ('people', 0.33985), ('windows', 0.28385), ('know', 0.26232), ('file', 0.18422)]\n",
      "Topic 14: [('space', 0.39946), ('think', 0.23258), ('know', 0.18074), ('nasa', 0.15174), ('problem', 0.12957)]\n",
      "Topic 15: [('space', 0.31613), ('good', 0.3094), ('card', 0.22603), ('people', 0.17476), ('time', 0.14496)]\n",
      "Topic 16: [('people', 0.48156), ('problem', 0.19961), ('window', 0.15281), ('time', 0.14664), ('game', 0.12871)]\n",
      "Topic 17: [('time', 0.34465), ('bike', 0.27303), ('right', 0.25557), ('windows', 0.1997), ('file', 0.19118)]\n",
      "Topic 18: [('time', 0.5973), ('problem', 0.15504), ('file', 0.14956), ('think', 0.12847), ('israel', 0.10903)]\n",
      "Topic 19: [('file', 0.44163), ('need', 0.26633), ('card', 0.18388), ('files', 0.17453), ('right', 0.15448)]\n",
      "Topic 20: [('problem', 0.33006), ('file', 0.27651), ('thanks', 0.23578), ('used', 0.19206), ('space', 0.13185)]\n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "def get_topics(components, feature_names, n=5):\n",
    "    for idx, topic in enumerate(components):\n",
    "        print(\"Topic %d:\" % (idx+1), [(feature_names[i], topic[i].round(5)) for i in topic.argsort()[:-n - 1:-1]])\n",
    "get_topics(svd_model.components_, terms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "literary-raise",
   "metadata": {},
   "source": [
    "# 5. LSA의 장단점\n",
    "쉽고 빠른 구현으로 단어의 잠재적 의미 도출   \n",
    "하지만 SVD 특성상 계산된 LSA에 새 데이터를 추가하려면 처음부터 다시 계산해야 함(새 정보 업데이트의 어려움)   \n",
    "이는 최근 LSA 대신 Word2Vec 등의 인공 신경망 기반 방법론이 각광받는 이유"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
