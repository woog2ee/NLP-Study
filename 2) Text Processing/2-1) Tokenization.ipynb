{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "minor-parallel",
   "metadata": {},
   "source": [
    "# 1. 단어 토큰화(Word Tokenization)\n",
    "word는 단어 외에도 단어구, 의미를 갖는 문자열로도 간주   \n",
    "구두점(punctuation)을 지운 뒤 whitespace를 기준으로 잘라냄   \n",
    "(구두점은 마침표(.), 컴마(,), 세미콜론(;), 느낌표(!) 같은 기호)   \n",
    "\n",
    "한국어는 띄어쓰기만으로 단어 토큰을 구분하기 어려움   \n",
    "토큰화는 cleaning만으로 해결되지 않으며, 구두점과 특수문자 제거시 토큰이 의미를 잃는 경우도 존재"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-parish",
   "metadata": {},
   "source": [
    "# 2. 토큰화 중 생기는 선택의 순간\n",
    "예상치 못한 경우로 토큰화의 기준을 생각해야 하는 경우 발생   \n",
    "데이터를 어떤 용도로 사용할 건지, 그에 영향이 없는 기준으로 선택"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "destroyed-gross",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr.', 'Jone', \"'s\", 'Orphange', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# NLTK에서의 어퍼스트로피 처리\n",
    "from nltk.tokenize import word_tokenize\n",
    "print(word_tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphange is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "blessed-cornell",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Don', \"'\", 't', 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', ',', 'Mr', '.', 'Jone', \"'\", 's', 'Orphange', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop', '.']\n"
     ]
    }
   ],
   "source": [
    "# WordPunctTokenizer에서의 어퍼스트로피 처리\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "print(WordPunctTokenizer().tokenize(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphange is as cheery as cheery goes for a pastry shop.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "major-survivor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"don't\", 'be', 'fooled', 'by', 'the', 'dark', 'sounding', 'name', 'mr', \"jone's\", 'orphange', 'is', 'as', 'cheery', 'as', 'cheery', 'goes', 'for', 'a', 'pastry', 'shop']\n"
     ]
    }
   ],
   "source": [
    "# keras로 어퍼스토로피 처리\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "print(text_to_word_sequence(\"Don't be fooled by the dark sounding name, Mr. Jone's Orphange is as cheery as cheery goes for a pastry shop.\"))\n",
    "# don't나 jone's의 같은 어퍼스트로피 보존 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-postcard",
   "metadata": {},
   "source": [
    "# 3. 토큰화에서 고려해야할 사항\n",
    "토큰화는 단순히 corpus에서 punctuation을 제외하고 whitespace로 잘라내는 작업만은 아님   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-ministry",
   "metadata": {},
   "source": [
    "> **1) 구두점이나 특수 문자를 단순 제외해서는 안 됨**   \n",
    "> 문장의 경계를 나타내는 마침표(.) 같은 경우 제외하지 않을 수 있음   \n",
    "> 단어 내에 구두점을 가진 경우(ph.D, AT&T 등), $과 /의 경우, 숫자 사이 콤마의 경우"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vertical-superior",
   "metadata": {},
   "source": [
    "> **2) 줄임말과 단어 내에 띄어쓰기의 경우**   \n",
    "> 어퍼스트로피로 문장의 접어(clitic)가 생긴 경우, 기존 문장은 띄어쓰기를 포함함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "multiple-oracle",
   "metadata": {},
   "source": [
    "> **3) 표준 토큰화 예제**   \n",
    "> **Penn Treebank Tokenization**\n",
    "> 1. 하이픈으로 구성된 단어는 하나로 유지한다.\n",
    "> 2. doesn't와 같이 어퍼스토로피로 접혀있던 단어는 분리한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "interpreted-cinema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Starting', 'a', 'home-based', 'restaurant', 'may', 'be', 'an', 'ideal', ',', 'it', 'does', \"n't\", 'have', 'a', 'food', 'chain', 'or', 'restaurant', 'of', 'their', 'own', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "\n",
    "text = \"Starting a home-based restaurant may be an ideal, it doesn't have a food chain or restaurant of their own.\"\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metric-chemistry",
   "metadata": {},
   "source": [
    "# 4. 문장 토큰화(Sentence Tokenization)\n",
    "문장 분류(sentence segmentation), corpus 내에서 문장 단위로 구분하는 작업   \n",
    "정제 안 된 corpus는 문장 단위로 구분되지 않았을 가능성 존재   \n",
    "\n",
    "!나 ?는 구분자(boundary) 역할을 하나 마침표는 문장의 끝이 아니더라도 등장 가능   \n",
    "그렇기에 사용하는 corpus가 어떤 국적의 언어인지에 따라 직접 규칙들을 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "advance-fishing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['His barber kept his word.', 'But keeping such a huge secret to himself was driving him crazy.', 'Finally, the barber went up a mountain and almost to the edge of a cliff.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"His barber kept his word. But keeping such a huge secret to himself was driving him crazy. Finally, the barber went up a mountain and almost to the edge of a cliff. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
    "print(sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "nuclear-prince",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am actively looking for ph.D. students.', 'and you are a ph.D student.']\n"
     ]
    }
   ],
   "source": [
    "# 문장 중간에 마침표가 여러 번 등장하는 경우\n",
    "from nltk.tokenize import sent_tokenize\n",
    "text = \"I am actively looking for ph.D. students. and you are a ph.D student.\"\n",
    "print(sent_tokenize(text))\n",
    "# NLTK는 단순히 마침표를 구분자로 하여 문장을 구분하지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mathematical-envelope",
   "metadata": {},
   "source": [
    "한국어 문장 토큰화 도구 KSS(Korean Sentence Splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broad-watson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['딥 러닝 자연어 처리가 재미있기는 합니다.', '그런데 문제는 영어보다 한국어로 할 때 너무 어려워요.', '농담 아니에요.', '이제 해보면 알 걸요?']\n"
     ]
    }
   ],
   "source": [
    "import kss\n",
    "\n",
    "text = '딥 러닝 자연어 처리가 재미있기는 합니다. 그런데 문제는 영어보다 한국어로 할 때 너무 어려워요. 농담 아니에요. 이제 해보면 알 걸요?'\n",
    "print(kss.split_sentences(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spatial-price",
   "metadata": {},
   "source": [
    "# 5. 이진 분류기(Binary Classifier)\n",
    "예외를 발생시키는 마침표 처리를 위해 입력에 따라 두 개의 클래스로 분류하는 binary classifier 사용   \n",
    "1. 마침표(.)가 단어의 일부분, 즉 약어(abbreivation)로 쓰이는 경우\n",
    "2. 마침표(.)가 문장의 boundary일 경우   \n",
    "\n",
    "마침표가 어떤 클래스에 속하는지 결정하기 위해 어떤 마침표가 주로 약어로 쓰이는지 알아야 함   \n",
    "이때 이진 분류기 구현에서 약어 사전 이용   \n",
    "문장 토큰화를 수행하는 오픈소스로는 NLTK, OpenNLP, Stanford CoreNLP, splitta, LingPipe 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-vanilla",
   "metadata": {},
   "source": [
    "# 6. 한국에서의 토큰화 어려움\n",
    "영어는 합성어나 줄임말에 대한 예외처리만 하면 whitespace를 기준삼는 토큰화를 수행해도 잘 작동   \n",
    "하지만 한국어는 띄어쓰기만으로는 토큰화를 하기 부족함   \n",
    "\n",
    "띄어쓰기 단위인 어절로 하는 토큰화는 한국어 NLP에서 지양됨, 어절 토큰화는 단어 토큰화와 다름   \n",
    "한국어는 영어와 달리 **교착어**(조사와 어미를 붙여 말을 만드는 언어)임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-shark",
   "metadata": {},
   "source": [
    "> **1) 한국어는 교착어이다.**   \n",
    "> 영어는 he/him, 한국어는 그가/그에게/그를/그와/그는    \n",
    "> 자연어 처리를 하다보면 한 단어에 조사들이 많아 번거로움, 대부분의 한국어 NLP에서 조사는 분리   \n",
    "\n",
    "\n",
    "> **한국어 토큰화는 형태소(morpheme) 토큰화를 수행**   \n",
    "> 1. **자립 형태소**: 접사, 어미, 조사와 상관없이 자립해 사용하는 형태소, 그 자체로 단어임   \n",
    "> 예) 체언(명사, 대명사, 수사), 수식언(관형사, 부사), 감탄사 \n",
    "> 2. **의존 형태소**: 다른 형태소와 결합해 사용하는 형태소   \n",
    "> 예) 접사, 어미, 조사, 어간"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-covering",
   "metadata": {},
   "source": [
    "> **2) 한국어는 띄어쓰기가 영어보다 잘 지켜지지 않는다.**   \n",
    "띄어쓰기가 틀렸거나 지켜지지 않는 corpus가 많음   \n",
    "한국어는 띄어쓰기가 지켜지지 않아도 글을 쉽게 이해할 수 있음   \n",
    "한국어(모아쓰기 방식)와 영어(풀어쓰기 방식)라는 특성의 차이에 기인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-edition",
   "metadata": {},
   "source": [
    "# 7. 품사 태깅(Part-of-speech Tagging)\n",
    "단어의 표기는 같더라도 품사에 따라 의미가 달라지기도 함   \n",
    "단어의 의미를 제대로 파악하기 위해 해당 단어가 어떤 품사로 쓰였는지 판단"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acceptable-boundary",
   "metadata": {},
   "source": [
    "# 8. NLTK와 KoNLPy를 이용한 영어, 한국어 토큰화 실습\n",
    "NLTK에서 영어 corpus의 품사 태깅을 하고자 Penn Treebank POS Tags 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "christian-delhi",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'actively', 'looking', 'for', 'ph.D.', 'students', '.', 'and', 'you', 'are', 'a', 'ph.D.', 'student', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "text = \"I am actively looking for ph.D. students. and you are a ph.D. student.\"\n",
    "print(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "armed-payroll",
   "metadata": {},
   "source": [
    "**Penn Treebank POG Tags**   \n",
    "PRP: 인칭 대명사,   \n",
    "VBP: 동사,   \n",
    "RB: 부사,   \n",
    "VBG: 현재 부사,   \n",
    "IN: 전치사,   \n",
    "NNP: 고유 명사,   \n",
    "NNS: 복수형 명사,   \n",
    "CC: 접속사,   \n",
    "DT: 관사"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "northern-sauce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('actively', 'RB'),\n",
       " ('looking', 'VBG'),\n",
       " ('for', 'IN'),\n",
       " ('ph.D.', 'JJ'),\n",
       " ('students', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('and', 'CC'),\n",
       " ('you', 'PRP'),\n",
       " ('are', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('ph.D.', 'JJ'),\n",
       " ('student', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "x = word_tokenize(text)\n",
    "pos_tag(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-prompt",
   "metadata": {},
   "source": [
    "KoNLPy에서 사용하는 형태소 분석기로 Okt(Open Korea Text), Mecab, Komoran, 한나눔(Hannanum), 꼬꼬마(Kkma) 사용   \n",
    "\n",
    "**KoNLPy 형태소 분석기 공통 메소드**   \n",
    "1. morphs: 형태소 추출\n",
    "2. pos: 품사 태깅\n",
    "3. nouns: 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "distinct-weapon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '한', '당신', ',', '연휴', '에는', '여행', '을', '가봐요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(okt.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "hairy-despite",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'Adverb'), ('코딩', 'Noun'), ('한', 'Josa'), ('당신', 'Noun'), (',', 'Punctuation'), ('연휴', 'Noun'), ('에는', 'Josa'), ('여행', 'Noun'), ('을', 'Josa'), ('가봐요', 'Verb')]\n"
     ]
    }
   ],
   "source": [
    "print(okt.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "early-civilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print(okt.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-struggle",
   "metadata": {},
   "source": [
    "각 형태소 분석기의 성능과 결과가 다르기에, 용도에 따라 어떤 형태소 분석기가 적절한지 판단하고 선택 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "quantitative-heath",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['열심히', '코딩', '하', 'ㄴ', '당신', ',', '연휴', '에', '는', '여행', '을', '가보', '아요']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Kkma\n",
    "kkma = Kkma()\n",
    "print(kkma.morphs(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "rough-final",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('열심히', 'MAG'), ('코딩', 'NNG'), ('하', 'XSV'), ('ㄴ', 'ETD'), ('당신', 'NP'), (',', 'SP'), ('연휴', 'NNG'), ('에', 'JKM'), ('는', 'JX'), ('여행', 'NNG'), ('을', 'JKO'), ('가보', 'VV'), ('아요', 'EFN')]\n"
     ]
    }
   ],
   "source": [
    "print(kkma.pos(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ordered-yemen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['코딩', '당신', '연휴', '여행']\n"
     ]
    }
   ],
   "source": [
    "print(kkma.nouns(\"열심히 코딩한 당신, 연휴에는 여행을 가봐요\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
