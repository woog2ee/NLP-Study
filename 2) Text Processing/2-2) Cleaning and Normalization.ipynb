{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "impaired-hometown",
   "metadata": {},
   "source": [
    "정제 작업은 토큰화 후에도 남은 노이즈들을 제거하고자 지속적으로 이루어짐   \n",
    "완벽한 정제 작업은 어려운 편이라 일종의 합의점을 도출\n",
    "- 정제(Cleaning): 노이즈 데이터 제거\n",
    "- 정규화(Normalization): 표현 방법이 다른 단어들을 통합해 같은 단어로 만듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coupled-anatomy",
   "metadata": {},
   "source": [
    "# 1. 규칙에 기반한 표기가 다른 단어들의 통합\n",
    "USA와 US는 같은 의미를 가지므로 한 단어로 정규화   \n",
    "이때 통합 방법에는 어간 추출(stemming)과 표제어 추출(lemmatization)이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aging-overhead",
   "metadata": {},
   "source": [
    "# 2. 대, 소문자 통합\n",
    "보통 대문자를 소문자로 변환하는 작업   \n",
    "고유명사와 같이 대소문자가 구분되는 경우도 존재   \n",
    "결국 예외 사항을 크게 고려하지 않고, 모든 corpus를 소문자로 바꾸는 것이 종종 더 실용적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "horizontal-compiler",
   "metadata": {},
   "source": [
    "# 3. 불필요한 단어의 제거(Removing Unnecessary Words)\n",
    "불용어 제거, 등장 빈도가 적거나 길이가 짧은 단어 제거   \n",
    "노이즈 데이터는 특수 문자들과 분석 목적에 맞지 않는 불필요 단어들을 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "normal-junction",
   "metadata": {},
   "source": [
    "> **1) 등장 빈도가 적은 단어(Removing Rare words)**   \n",
    "> 방대한 데이터에서 적게 등장한 단어는 직관적인 분류에 도움이 되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intended-allah",
   "metadata": {},
   "source": [
    "> **2) 길이가 짧은 단어(Removing words with very a Short length)**   \n",
    "> 영어권에서는 길이가 짧은 단어의 삭제만으로 무의미한 단어들 제거에 효과적   \n",
    "> 길이를 조건으로 텍스트를 삭제하며 구두점까지도 한꺼번에 제거   \n",
    "> 하지만 한국어에서는 크게 유효하지 않음   \n",
    "\n",
    "> 영단어가 한국어 단어보다 평균적으로 긴데,   \n",
    "> 이는 영단어와 한국어 단어에서 한 글자가 가진 의미의 크기가 다르다는 점에서 기인함   \n",
    "> 한글은 함축적이나, 영어는 알파벳이 모여 단어를 완성함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "identified-williams",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " was wondering anyone out there could enlighten this car.\n"
     ]
    }
   ],
   "source": [
    "# 길이가 1~2인 단어들을 정규 표현식으로 삭제\n",
    "import re\n",
    "text = \"I was wondering if anyone out there could enlighten me on this car.\"\n",
    "shortword = re.compile(r'\\W*\\b\\w{1,2}\\b')\n",
    "print(shortword.sub('', text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acoustic-tribune",
   "metadata": {},
   "source": [
    "# 4. 정규 표현식(Regular Expression)\n",
    "노이즈 데이터의 특징을 잡아낸다면 정규 표현식을 통해 이를 제거 가능   \n",
    "예) HTML 문서에서 가져왔다면 HTML 태그, 뉴스 기사라면 기사 게재 시간 등   \n",
    "정규 표현식은 corpus 내에 계속 등장하는 글자들을 규칙에 기반해 한 번에 제거"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
