{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "tender-mailing",
   "metadata": {},
   "source": [
    "# 1. ReLU\n",
    "기울기 소실을 막고자 은닉층에 ReLU 변형 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "racial-scheme",
   "metadata": {},
   "source": [
    "# 2. Gradient Clipping\n",
    "기울기 폭주를 막고자 임계값을 넘는 기울기를 자름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "answering-separation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import optimizers\n",
    "Adam = optimizers.Adam(lr=0.0001, clipnorm=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-service",
   "metadata": {},
   "source": [
    "# 3. Weight Initialization\n",
    "> **1) Xavier Initialization**   \n",
    "> 여러 층의 기울기 분산 사이 균형을 맞추며 Sigmoid나 tanh에 좋은 성능\n",
    "\n",
    "> Uniform Distribution\n",
    "> $$W\\,\\tilde\\,Uniform(-\\sqrt{\\frac{6}{n_{in} + n_{out}}}, \\sqrt{\\frac{6}{n_{in} + n_{out}}})$$\n",
    "\n",
    "> Normal Distribution   \n",
    "> $$\\sigma = \\sqrt{\\frac{2}{n_{in} + n_{out}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tutorial-arcade",
   "metadata": {},
   "source": [
    "> **2) He Initialization**   \n",
    "> 이전 층 뉴런만 반영하며 ReLU 계열에 좋은 성능   \n",
    "\n",
    "> Uniform Distribution\n",
    "> $$W\\,\\tilde\\,Uniform(-\\sqrt{\\frac{6}{n_{in}}}, \\sqrt{\\frac{6}{n_{in}}})$$\n",
    "\n",
    "> Normal Distribution\n",
    "> $$\\sigma = \\sqrt{\\frac{2}{n_{in}}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "backed-answer",
   "metadata": {},
   "source": [
    "# 4. Batch Normalization\n",
    "> **1) Internal Covariate Shift**   \n",
    "> 신경망 층 사이에서 발생한 입력 데이터 분포 변화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outer-healing",
   "metadata": {},
   "source": [
    "> **2) Batch Normalization**   \n",
    "> 입력 데이터의 정규화 후 스케일과 시프트 수행\n",
    "> $$x^{(i)} \\leftarrow \\frac{x^{(i)} - \\mu_{\\beta}}{\\sqrt{\\sigma_{B}^{2} + \\epsilon}}$$\n",
    "> $$y^{(i)} \\leftarrow \\gamma x^{(i)} + \\beta = BN_{\\gamma, \\beta}(x^{(i)})$$\n",
    "\n",
    "> 가중치 초기화에 덜 민감해지며 학습 속도 개선   \n",
    "> 데이터로 인한 과적합을 방지하나 모델 복잡도를 높힘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stretch-cowboy",
   "metadata": {},
   "source": [
    "> **3) 한계**   \n",
    "> 미니 배치 크기에 의존적, 시점마다 다른 통계치를 가진 RNN에 적용하기 힘듦"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-means",
   "metadata": {},
   "source": [
    "# 5. Layer Normalization\n",
    "Mini Batch Normalization\n",
    "![Mini Batch Normalization](https://wikidocs.net/images/page/61375/%EB%B0%B0%EC%B9%98%EC%A0%95%EA%B7%9C%ED%99%94.PNG \"Mini Batch Normalization\")\n",
    "\n",
    "Layer Normalization\n",
    "![Layer Normalization](https://wikidocs.net/images/page/61375/%EC%B8%B5%EC%A0%95%EA%B7%9C%ED%99%94.PNG \"Layer Normalization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
