{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "parallel-conversation",
   "metadata": {},
   "source": [
    "# 1. Neural Network Language Model\n",
    "예문 \"what will the fat cat sit on\"에서 다음 단어 예측   \n",
    "\n",
    "단어 원-핫 인코딩   \n",
    "what = [1, 0, 0, 0, 0, 0, 0]   \n",
    "will = [0, 1, 0, 0, 0, 0, 0]   \n",
    "the = [0, 0, 1, 0, 0, 0, 0]   \n",
    "fat = [0, 0, 0, 1, 0, 0, 0]   \n",
    "cat = [0, 0, 0, 0, 1, 0, 0]   \n",
    "sit = [0, 0, 0, 0, 0, 1, 0]   \n",
    "on = [0, 0, 0, 0, 0, 0, 1]   \n",
    "\n",
    "NNLM은 4개 층으로 정해진 n개 단어만 참고   \n",
    "![Neural Network Language Model](https://wikidocs.net/images/page/45609/nnlm1.PNG \"Neural Network Language Model\")\n",
    "\n",
    "투사층은 원-핫 벡터를 작은 차원 벡터로 lookup   \n",
    "![lookup table](https://wikidocs.net/images/page/45609/nnlm2_renew.PNG  \"lookup table\")\n",
    "\n",
    "lookup된 모든 임베딩 벡터들은 연결됨\n",
    "![lookup table2](https://wikidocs.net/images/page/45609/nnlm3_renew.PNG \"lookup table2\")\n",
    "\n",
    "은닉층과 출력층을 지나면, 역전파가 이루어져 임베딩 벡터와 가중치 행렬이 학습됨\n",
    "![lookup table3](https://wikidocs.net/images/page/45609/nnlm5_final.PNG \"lookup table3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-contamination",
   "metadata": {},
   "source": [
    "# 2. NNLM 이점과 한계\n",
    "> **1) 기존 모델 개선**   \n",
    "> 밀집 벡터를 사용해 희소 문제 해결 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-cambridge",
   "metadata": {},
   "source": [
    "> **2) 고정 길이 입력**   \n",
    "> N-gram과 마찬가지로 정해진 n개 단어만 참고"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
