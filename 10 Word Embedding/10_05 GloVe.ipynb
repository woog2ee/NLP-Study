{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acoustic-bulgaria",
   "metadata": {},
   "source": [
    "# 1. 기존 방법 비판\n",
    "**LSA**는 전체 통계 정보 반영, 단어 의미 유추에 어려움   \n",
    "**Word2Vec**은 단어 의미 유추, 전체 통계 정보 반영 어려움   \n",
    "**GloVe**(Global Vectors for Word Representation)는 카운트와 예측 기반 모두 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "encouraging-dealer",
   "metadata": {},
   "source": [
    "# 2. Window based Co-Occurence Matrix\n",
    "전체 단어 집합으로 윈도우 크기에 따라 행렬 구성   \n",
    "\n",
    "윈도우 크기 1일 때,   \n",
    "문장1: I like deep learning   \n",
    "문장2: I like NLP   \n",
    "문장3: I enjoy flying   \n",
    "\n",
    "카운트|I|like|enjoy|deep|learning|NLP|flying\n",
    ":--|:--|:--|:--|:--|:--|:--|:--\n",
    "I|0|2|1|0|0|0|0\n",
    "like|2|0|0|1|0|1|0\n",
    "enjoy|1|0|0|0|0|0|1\n",
    "deep|0|1|0|0|1|0|0\n",
    "learning|0|0|0|1|0|0|0\n",
    "NLP|0|1|0|0|0|0|0\n",
    "flying|0|0|1|0|0|0|0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-colors",
   "metadata": {},
   "source": [
    "# 3. Co-Occurence Probability\n",
    "동시 등장 행렬에서 특정 단어 전체 등장 횟수와, 등장시 어떤 단어가 등장한 횟수의 조건부 확률\n",
    "\n",
    "동시 등장 확률과 크기 관계 비|k=solid|k=gas|k=water|k=fashion\n",
    ":--|:--|:--|:--|:--\n",
    "P(ice 등장시 k 등장 확률)|큰 값|작은 값|큰 값|작은 값\n",
    "P(steam 등장시 k 등장 확률)|작은 값|큰 값|큰 값|작은 값\n",
    "P(ice 등장시 k 등장 확률) / P(steam 등장시 k 등장 확률)|큰 값|작은 값|1에 가까움|1에 가까움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handmade-viking",
   "metadata": {},
   "source": [
    "# 4. Loss function\n",
    "동시 등장 확률은 중심 단어와 주변 단어 임베딩 벡터 내적   \n",
    "(P는 중심 단어 i 등장시 윈도우 내 주변 단어 k 등장 확률)\n",
    "$$w_{i} \\cdot \\tilde{w_{k}} \\approx P(k\\,|\\,i) = P_{ik}$$\n",
    "\n",
    "함수 F에 벡터 적용시, 동시 등장 확률 비가 나오는 초기 식 가정\n",
    "$$F(w_{i}, w_{j}, \\tilde{w_{k}}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "1. F는 동시 등장 확률 비를 벡터 공간에 인코딩하는 목적\n",
    "$$F(w_{i} - w_{j}, \\tilde{w_{k}}) = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "2. 좌변은 벡터고 우변은 스칼라임을 성립해주는 내적   \n",
    "(선형 공간에서 단어 의미 관계를 표현하고자 뺄셈과 내적 사용)\n",
    "$$F((w_{i} - w_{j})^{T}\\tilde{w_{k}}) = \\frac{P_{ik}}{P_{jk}}$$   \n",
    "\n",
    "3. 이때 F는 준동형(Homomorphism) 만족   \n",
    "$$F(a + b) = F(a)F(b),\\,F(a - b) = \\frac{F(a)}{F(b)},\\,\\forall{a,b} \\in \\mathbb{R}$$\n",
    "\n",
    "적용시\n",
    "$$F((w_{i} - w_{j})^{T}\\tilde{w_{k}}) = \\frac{F(w_{i}^{T}\\tilde{w_{k}})}{F(w_{j}^{T}\\tilde{w_{k}})} = \\frac{P_{ik}}{P_{jk}}$$\n",
    "\n",
    "이를 만족하는 F는 Exponential function   \n",
    "(X는 중심 단어 i 등장시 윈도우 내 주변 단어 k 등장 횟수)\n",
    "$$exp(w_{i}^{T}\\tilde{w_{k}} - w_{j}^{T}\\tilde{w_{k}}) = \\frac{exp(w_{i}^T\\tilde{w_{k}})}{exp(w_{j}^T\\tilde{w_{k}})} = \\frac{P_{ik}}{P_{jk}}$$\n",
    "$$exp(w_{i}^{T}\\tilde{w_{k}}) = P_{ik} = \\frac{X_{ik}}{X_{i}}$$\n",
    "$$w_{i}^{T}\\tilde{w_{k}} = logP_{ik} = log(\\frac{X_{ik}}{X_{i}}) = logX_{ik} - logX_{i}$$\n",
    "\n",
    "각 임베딩 벡터 위치를 바꾸어도 식이 성립하고자, 로그항을 편향으로 대체\n",
    "$$w_{i}^{T}\\tilde{w_{k}} + b_{i} + \\tilde{b_{k}} = logX_{ik}$$\n",
    "\n",
    "손실 함수는 이 차이를 최소화하는 일반화 식\n",
    "$$Loss function = \\sum_{m,n=1}^{V}(w_{m}^{T} + b_{m} + \\tilde{b_{n}} - logX_{mn})^{2}$$\n",
    "   \n",
    "희소 행렬에서 낮은 빈도수에 가중치를 주는 weight function 도입, X가 클 때 지나친 가중치를 막고자 최댓값 설정   \n",
    "$$f(x) = min(1, (\\frac{x}{x_{max}})^{\\frac{3}{4}})$$\n",
    "![weight function](https://wikidocs.net/images/page/22885/%EA%B0%80%EC%A4%91%EC%B9%98.PNG \"weight function\")\n",
    "\n",
    "최종 손실 함수\n",
    "$$Loss function = \\sum_{m,n=1}^{V}\\,f(X_{mn})(w_{m}^{T}\\tilde{w_{n}} + b_{m} + \\tilde{b_{n}} - logX_{mn})^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-administrator",
   "metadata": {},
   "source": [
    "# 5. GloVe 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "therapeutic-buddy",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('ted_en-20160408.xml', <http.client.HTTPMessage at 0x1e5cecf0e20>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "import urllib.request\n",
    "import zipfile\n",
    "from lxml import etree\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "urllib.request.urlretrieve(\"https://raw.githubusercontent.com/GaoleMeng/RNN-and-FFNN-textClassification/master/ted_en-20160408.xml\",\n",
    "                           filename=\"ted_en-20160408.xml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "legislative-support",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "targetXML = open('ted_en-20160408.xml', 'r', encoding='UTF8')\n",
    "target_text = etree.parse(targetXML)\n",
    "\n",
    "parse_text = '\\n'.join(target_text.xpath('//content/text()'))\n",
    "content_text = re.sub(r'\\([^)]*\\)', '', parse_text)\n",
    "\n",
    "sent_text = sent_tokenize(content_text)\n",
    "\n",
    "normalized_text = []\n",
    "for string in sent_text:\n",
    "     tokens = re.sub(r\"[^a-z0-9]+\", \" \", string.lower())\n",
    "     normalized_text.append(tokens)\n",
    "\n",
    "result = [word_tokenize(sentence) for sentence in normalized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "incorrect-vintage",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 20 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n"
     ]
    }
   ],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "# 동시 등장 행렬 생성\n",
    "corpus = Corpus()\n",
    "corpus.fit(result, window=5)\n",
    "\n",
    "glove = Glove(no_components=100, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=20, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "recognized-layer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('woman', 0.9648368487300606), ('guy', 0.87853441458273), ('girl', 0.8534987437931901), ('young', 0.846071380470317)]\n"
     ]
    }
   ],
   "source": [
    "model_result1 = glove.most_similar(\"man\")\n",
    "print(model_result1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adequate-junction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('girl', 0.9476923851537493), ('woman', 0.8419994258815078), ('kid', 0.8361017051820097), ('man', 0.8279171755525907)]\n"
     ]
    }
   ],
   "source": [
    "model_result2 = glove.most_similar(\"boy\")\n",
    "print(model_result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "suburban-specification",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('harvard', 0.888141053853674), ('mit', 0.8571900561051062), ('cambridge', 0.8353162335157012), ('stanford', 0.8343890928881722)]\n"
     ]
    }
   ],
   "source": [
    "model_result3 = glove.most_similar(\"university\")\n",
    "print(model_result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "willing-draft",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('clean', 0.8436805363087417), ('air', 0.8276533018109319), ('electricity', 0.818030617567804), ('fresh', 0.8173633565603985)]\n"
     ]
    }
   ],
   "source": [
    "model_result4 = glove.most_similar(\"water\")\n",
    "print(model_result4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "arctic-setting",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('chemistry', 0.8938172020985033), ('economics', 0.8825295341379408), ('beauty', 0.8640229385013825), ('mathematics', 0.8553610296644241)]\n"
     ]
    }
   ],
   "source": [
    "model_result5 = glove.most_similar(\"physics\")\n",
    "print(model_result5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ongoing-quantity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tissue', 0.8488022951766478), ('nerve', 0.836366071812534), ('stem', 0.7888055281526625), ('channel', 0.7717473953205604)]\n"
     ]
    }
   ],
   "source": [
    "model_result6 = glove.most_similar(\"muscle\")\n",
    "print(model_result6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "brief-combine",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('water', 0.8436805363087417), ('fresh', 0.8321380109071955), ('wind', 0.8010232560853364), ('heat', 0.7796518615106788)]\n"
     ]
    }
   ],
   "source": [
    "model_result7 = glove.most_similar(\"clean\")\n",
    "print(model_result7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
