{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "native-airplane",
   "metadata": {},
   "source": [
    "# 1. Negative Sampling\n",
    "Word2Vec은 단어 집합이 수만 이상이면 학습에 무거움   \n",
    "주변 단어 임베딩 벡터까지 업데이트는 비효율적   \n",
    "Negative Sampling은 일부 단어 집합에만 집중"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "educational-dietary",
   "metadata": {},
   "source": [
    "# 2. Skip-Gram with Negative Sampling\n",
    "Skip-gram은 중심 단어로 주변 단어 예측\n",
    "![Skip-gram](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC1-1.PNG \"Skip-gram\")\n",
    "\n",
    "SGNS는 중심/주변 단어로 서로 이웃 관계일 확률 예측\n",
    "![SGNS](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC1-2.PNG \"SGNS\")\n",
    "  \n",
    "중심 단어에 대한 작은 단어 집합을 통해 마지막을 이진 분류로 변환\n",
    "![SGNS](https://wikidocs.net/images/page/69141/%EA%B7%B8%EB%A6%BC4.PNG \"SGNS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wicked-orleans",
   "metadata": {},
   "source": [
    "# 3. 20뉴스그룹 데이터 전처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dominant-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "determined-prague",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수:  11314\n"
     ]
    }
   ],
   "source": [
    "dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))\n",
    "documents = dataset.data\n",
    "print('총 샘플 수: ', len(documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "pleasant-calcium",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-d3b8212a83b0>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \")\n"
     ]
    }
   ],
   "source": [
    "news_df = pd.DataFrame({'document': documents})\n",
    "\n",
    "news_df['clean_doc'] = news_df['document'].str.replace(\"[^a-zA-Z]\", \" \") # 특수문자 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3])) # 길이 3 이하 단어 제거\n",
    "news_df['clean_doc'] = news_df['clean_doc'].apply(lambda x: x.lower()) # 소문자 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adaptive-situation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# null값 유무 확인\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "elder-pittsburgh",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_df.replace(\"\", float(\"NaN\"), inplace=True)\n",
    "news_df.isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "sixth-world",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "빈 값 제거 후 총 샘플 수:  10995\n"
     ]
    }
   ],
   "source": [
    "news_df.dropna(inplace=True)\n",
    "print('빈 값 제거 후 총 샘플 수: ', len(news_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "continuing-signature",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 불용어 제거\n",
    "stop_words = stopwords.words('english')\n",
    "tokenized_doc = news_df['clean_doc'].apply(lambda x: x.split())\n",
    "tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "tokenized_doc = tokenized_doc.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "weird-assistant",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 샘플 수:  10940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "# 단어가 1개 이하인 인덱스 저장 후 해당 단어 제거\n",
    "drop_train = [index for index, sentence in enumerate(tokenized_doc) if len(sentence)<=1]\n",
    "tokenized_doc = np.delete(tokenized_doc, drop_train, axis=0)\n",
    "print('총 샘플 수: ', len(tokenized_doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "muslim-executive",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 집합 생성\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(tokenized_doc)\n",
    "\n",
    "# 정수 인코딩\n",
    "word2idx = tokenizer.word_index\n",
    "idx2word = {v:k for k, v in word2idx.items()}\n",
    "encoded = tokenizer.texts_to_sequences(tokenized_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "compressed-millennium",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9, 59, 603, 207, 3278, 1495, 474, 702, 9470, 13686, 5533, 15227, 702, 442, 702, 70, 1148, 1095, 1036, 20294, 984, 705, 4294, 702, 217, 207, 1979, 15228, 13686, 4865, 4520, 87, 1530, 6, 52, 149, 581, 661, 4406, 4988, 4866, 1920, 755, 10668, 1102, 7837, 442, 957, 10669, 634, 51, 228, 2669, 4989, 178, 66, 222, 4521, 6066, 68, 4295], [1026, 532, 2, 60, 98, 582, 107, 800, 23, 79, 4522, 333, 7838, 864, 421, 3825, 458, 6488, 458, 2700, 4730, 333, 23, 9, 4731, 7262, 186, 310, 146, 170, 642, 1260, 107, 33568, 13, 985, 33569, 33570, 9471, 11491]]\n"
     ]
    }
   ],
   "source": [
    "print(encoded[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "frequent-electronics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 집합 크기:  64277\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "print('단어 집합 크기: ', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "premier-trust",
   "metadata": {},
   "source": [
    "# 4. 데이터셋 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "express-phenomenon",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import skipgrams\n",
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded[:10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "timely-recommendation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(makes (228), acts (1102) -> 1)\n",
      "(seem (207), reconfig (63295) -> 0)\n",
      "(reputation (5533), liptrap (36153) -> 0)\n",
      "(europe (1095), israels (13686) -> 1)\n",
      "(media (702), sargisian (21387) -> 0)\n"
     ]
    }
   ],
   "source": [
    "pairs, labels = skip_grams[0][0], skip_grams[0][1]\n",
    "for i in range(5):\n",
    "    print('({:s} ({:d}), {:s} ({:d}) -> {:d})'.format(idx2word[pairs[i][0]], pairs[i][0],\n",
    "                                                      idx2word[pairs[i][1]], pairs[i][1],\n",
    "                                                      labels[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "placed-surface",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전체 샘플 수:  10\n"
     ]
    }
   ],
   "source": [
    "print('전체 샘플 수: ', len(skip_grams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "honey-chamber",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2220\n",
      "2220\n"
     ]
    }
   ],
   "source": [
    "print(len(pairs))\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "confirmed-suggestion",
   "metadata": {},
   "outputs": [],
   "source": [
    "skip_grams = [skipgrams(sample, vocabulary_size=vocab_size, window_size=10) for sample in encoded]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "taken-cattle",
   "metadata": {},
   "source": [
    "# 5. SGNS 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "conscious-relief",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Embedding, Reshape, Activation, Input\n",
    "from tensorflow.keras.layers import Dot\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from IPython.display import SVG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "signal-least",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "excited-policy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중심 단어 임베딩 테이블\n",
    "w_inputs = Input(shape=(1,), dtype='int32')\n",
    "word_embedding = Embedding(vocab_size, embed_size)(w_inputs)\n",
    "\n",
    "# 주변 단어 임베딩 테이블\n",
    "c_inputs = Input(shape=(1,), dtype='int32')\n",
    "context_embedding = Embedding(vocab_size, embed_size)(c_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "interesting-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측값\n",
    "dot_product = Dot(axes=2)([word_embedding, context_embedding])\n",
    "dot_product = Reshape((1,), input_shape=(1,1))(dot_product)\n",
    "output = Activation('sigmoid')(dot_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "conscious-money",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1, 100)       6427700     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1, 100)       6427700     input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dot (Dot)                       (None, 1, 1)         0           embedding[0][0]                  \n",
      "                                                                 embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "reshape (Reshape)               (None, 1)            0           dot[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           reshape[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,855,400\n",
      "Trainable params: 12,855,400\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "('Failed to import pydot. You must `pip install pydot` and install graphviz (https://graphviz.gitlab.io/download/), ', 'for `pydotprint` to work.')\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=[w_inputs, c_inputs], outputs=output)\n",
    "model.summary()\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "plot_model(model, to_file='model3.png', show_shapes=True, show_layer_names=True, rankdir='TB')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "electronic-nickel",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 Loss:  4630.118759036064\n",
      "Epoch:  2 Loss:  3669.674777057022\n",
      "Epoch:  3 Loss:  3508.549609189853\n",
      "Epoch:  4 Loss:  3305.6848546154797\n",
      "Epoch:  5 Loss:  3080.794129396789\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 6):\n",
    "    loss = 0\n",
    "    for _, elem in enumerate(skip_grams):\n",
    "        first_elem = np.array(list(zip(*elem[0]))[0], dtype='int32')\n",
    "        second_elem = np.array(list(zip(*elem[0]))[1], dtype='int32')\n",
    "        labels = np.array(elem[1], dtype='int32')\n",
    "        X = [first_elem, second_elem]\n",
    "        Y = labels\n",
    "        loss += model.train_on_batch(X,Y)\n",
    "    print('Epoch: ', epoch, 'Loss: ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "relevant-geography",
   "metadata": {},
   "source": [
    "# 6. 결과 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "accepted-melbourne",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aging-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('vectors.txt', 'w')\n",
    "f.write('{} {}\\n'.format(vocab_size-1, embed_size))\n",
    "vectors = model.get_weights()[0]\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    f.write('{} {}\\n'.format(word, ' '.join(map(str, list(vectors[i, :])))))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "neutral-transmission",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = gensim.models.KeyedVectors.load_word2vec_format('./vectors.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "informal-library",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hamas', 0.8299499750137329),\n",
       " ('murdered', 0.8009821176528931),\n",
       " ('civilians', 0.7931509017944336),\n",
       " ('massacred', 0.7924225330352783),\n",
       " ('occupied', 0.7918510437011719),\n",
       " ('treaty', 0.7897014021873474),\n",
       " ('wounded', 0.7887318134307861),\n",
       " ('flee', 0.7853697538375854),\n",
       " ('killed', 0.7793642282485962),\n",
       " ('civilian', 0.7762709856033325)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['soldiers'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "portuguese-billion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('blood', 0.5624978542327881),\n",
       " ('stomach', 0.5467677116394043),\n",
       " ('diet', 0.5446897149085999),\n",
       " ('pain', 0.5442729592323303),\n",
       " ('coated', 0.5351646542549133),\n",
       " ('parasites', 0.5261176824569702),\n",
       " ('disease', 0.5237654447555542),\n",
       " ('hurting', 0.5218662023544312),\n",
       " ('medicine', 0.5195770263671875),\n",
       " ('nose', 0.5111726522445679)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['doctor'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "requested-favor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('incontrovertibly', 0.6089888215065002),\n",
       " ('officers', 0.6009635925292969),\n",
       " ('agencies', 0.5896414518356323),\n",
       " ('authorities', 0.5895517468452454),\n",
       " ('tanks', 0.5851243138313293),\n",
       " ('homicide', 0.5835860967636108),\n",
       " ('constitution', 0.5808992385864258),\n",
       " ('ataman', 0.5798817276954651),\n",
       " ('federal', 0.5789436101913452),\n",
       " ('abroad', 0.5747097730636597)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['police'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "continuing-eight",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('sinned', 0.685368001461029),\n",
       " ('realised', 0.6591967344284058),\n",
       " ('fantasies', 0.656895637512207),\n",
       " ('discriminatory', 0.642410397529602),\n",
       " ('emotional', 0.6338437795639038),\n",
       " ('neighbors', 0.6330261826515198),\n",
       " ('backs', 0.630382776260376),\n",
       " ('fallen', 0.6239956617355347),\n",
       " ('amusing', 0.6238863468170166),\n",
       " ('buried', 0.6236354112625122)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['knife'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "tutorial-powell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rebuilt', 0.6259212493896484),\n",
       " ('seat', 0.5542675852775574),\n",
       " ('mufflers', 0.5221957564353943),\n",
       " ('bike', 0.5119962692260742),\n",
       " ('helmet', 0.5111349821090698),\n",
       " ('tires', 0.508537232875824),\n",
       " ('rack', 0.5048271417617798),\n",
       " ('metzeler', 0.5013248920440674),\n",
       " ('sucker', 0.5009517669677734),\n",
       " ('bmws', 0.49873510003089905)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v.most_similar(positive=['engine'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
