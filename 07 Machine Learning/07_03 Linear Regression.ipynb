{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "visible-german",
   "metadata": {},
   "source": [
    "# 1. Linear Regression\n",
    "독립변수에 따른 종속변수의 선형 관계\n",
    "> **1) Linear Regression**   \n",
    "$$y = Wx + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-wallace",
   "metadata": {},
   "source": [
    "> **2) Multiple Linear Regression**   \n",
    "> $$y = W_{1}x_{1} + W_{2}x_{2} + ... + W_{n}x_{n} + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "facial-removal",
   "metadata": {},
   "source": [
    "# 2. Hypothesis\n",
    "변수 관계를 유추하는 식\n",
    "$$H(x) = Wx + b$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agreed-ecuador",
   "metadata": {},
   "source": [
    "# 3. Cost Function: MSE\n",
    "회귀는 주로 Mean Squared Error 사용\n",
    "$$cost(W, b) = \\frac{1}{n}\\sum_{i=1}^{n}(y^{(i)} - H(x^{(i)}))^{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "square-portuguese",
   "metadata": {},
   "source": [
    "# 4. Optimizer: Gradient Descent \n",
    "Gradient Descent는 비용 함수에서 접선 기울기가 0인 지점을 찾아감   \n",
    "Learning Rate는 이때 변수를 얼마나 크게 변경할지 결정\n",
    "$$W := W - \\alpha\\frac{\\partial}{\\partial{W}}cost(W)$$\n",
    "![Gradient Descent](https://wikidocs.net/images/page/21670/%EC%A0%91%EC%84%A0%EC%9D%98%EA%B8%B0%EC%9A%B8%EA%B8%B01.PNG \"Gradient Descent\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
